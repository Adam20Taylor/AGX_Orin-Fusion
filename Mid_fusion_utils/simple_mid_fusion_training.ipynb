{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6aa5623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/adatay20/env23/fusion_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/nfs/home/adatay20/env23/fusion_env/lib/python3.8/site-packages/MinkowskiEngine-0.5.4-py3.8-linux-x86_64.egg/MinkowskiEngine/__init__.py:36: UserWarning: The environment variable `OMP_NUM_THREADS` not set. MinkowskiEngine will automatically set `OMP_NUM_THREADS=16`. If you want to set `OMP_NUM_THREADS` manually, please export it on the command line before running a python script. e.g. `export OMP_NUM_THREADS=12; python your_program.py`. It is recommended to set it below 24.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import MinkowskiEngine as ME\n",
    "#from torchvision import transforms\n",
    "#from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from MinkowskiEngine.utils import sparse_quantize\n",
    "import traceback\n",
    "from pytorch3d.ops import knn_points\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c9a2fc",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a11e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6052\n"
     ]
    }
   ],
   "source": [
    "scannet_dir = './scannet_train_detection_data'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scannet_files = os.listdir(scannet_dir)\n",
    "print(len(scannet_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed81c32d-c07b-402e-86ff-0ec44c418d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def create_split():\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Regex to match ScanNet-style scene IDs (e.g., scene0000_00)\n",
    "    scene_id_pattern = re.compile(r'(scene\\d{4}_\\d{2})')\n",
    "    \n",
    "    # Gather all scene IDs\n",
    "    scene_ids = set()\n",
    "    for fname in scannet_files:\n",
    "        if fname.endswith(\".npy\"):\n",
    "            match = scene_id_pattern.match(fname)\n",
    "            if match:\n",
    "                scene_ids.add(match.group(1))\n",
    "    \n",
    "    # Convert to list and shuffle\n",
    "    scene_ids = list(scene_ids)\n",
    "    random.shuffle(scene_ids)\n",
    "    \n",
    "    # Split (80% train, 10% val, 10% test)\n",
    "    n = len(scene_ids)\n",
    "    train_end = int(0.8 * n)\n",
    "    val_end = int(0.9 * n)\n",
    "    \n",
    "    train_ids = scene_ids[:train_end]\n",
    "    val_ids = scene_ids[train_end:val_end]\n",
    "    test_ids = scene_ids[val_end:]\n",
    "    \n",
    "    # Save each split to its own text file\n",
    "    with open(\"train.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(train_ids))\n",
    "    \n",
    "    with open(\"val.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(val_ids))\n",
    "    \n",
    "    with open(\"test.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(test_ids))\n",
    "    \n",
    "    print(f\"Train: {len(train_ids)}, Val: {len(val_ids)}, Test: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa5ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScanNetDetectionDataset(Dataset):\n",
    "    def __init__(self, data_dir, split_list, voxel_size=0.2, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Path to the directory containing the preprocessed .npy files.\n",
    "            split_list (list): List of scene IDs to load (e.g., ['scene0000_00', 'scene0001_00']).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            \n",
    "        Returns:\n",
    "            sparse_vert (SparseTensor): SparseTensor that has xyz and rgb features of given scene, used for TR3D\n",
    "            coords_dict (Tensor): Only xyz coordinates of given scene, used for VoteNet\n",
    "            bbox (Tensor): Ground truth bounding boxes for the given scene\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.split_list = split_list\n",
    "        self.transform = transform\n",
    "        self.voxel_size = voxel_size\n",
    "        self.class_map = {\n",
    "        3 : 0,\n",
    "        4 : 1,\n",
    "        5 : 2,\n",
    "        6 : 3,\n",
    "        7 : 4,\n",
    "        8 : 5,\n",
    "        9 : 6,\n",
    "        10 : 7,\n",
    "        11 : 8,\n",
    "        12 : 9,\n",
    "        14 : 10,\n",
    "        16 : 11,\n",
    "        24 : 12,\n",
    "        28 : 13,\n",
    "        33 : 14,\n",
    "        34 : 15,\n",
    "        36 : 16,\n",
    "        39 : 17\n",
    "    }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene_id = self.split_list[idx]\n",
    "\n",
    "        # Load files\n",
    "        vert = np.load(os.path.join(self.data_dir, f\"{scene_id}_vert.npy\"))            # (N, 6)\n",
    "            \n",
    "        vert = normalize_points_color(vert)\n",
    "        vert = sample_points(vert, 0.33)\n",
    "        coords = vert[:,0:3]                                                            # (N, 3)\n",
    "        features = vert[:,3:]\n",
    "\n",
    "        coords = torch.from_numpy(coords)\n",
    "        \n",
    "        bbox = np.load(os.path.join(self.data_dir, f\"{scene_id}_bbox.npy\"))  # (K, 7)\n",
    "        if self.transform:\n",
    "            coords, bbox = self.transform(coords, bbox)\n",
    "\n",
    "        \n",
    "        if bbox.shape[0] > 0:\n",
    "            class_ids = bbox[:, 6]\n",
    "            \n",
    "            map_func = np.vectorize(lambda x: self.class_map.get(x, x))  # fallback to x if not in map\n",
    "            new_class_ids = map_func(class_ids)\n",
    "            bbox[:, 6] = new_class_ids\n",
    "\n",
    "        bbox = torch.from_numpy(bbox).float()\n",
    "\n",
    "        # Convert to tensors\n",
    "        coords_with_dummy = torch.hstack([coords, torch.zeros((coords.shape[0], 1))])\n",
    "        batched_coords = torch.unsqueeze(coords_with_dummy, axis=0)\n",
    "        votenet_coords = batched_coords.float()        \n",
    "        # TR3D Preprocessing\n",
    "        feats = torch.from_numpy(features)\n",
    "\n",
    "        coords = torch.floor(coords / self.voxel_size).int()\n",
    "\n",
    "        return coords, feats, votenet_coords, bbox\n",
    "    \n",
    "def get_data(file_name, voxel_size, transform=None):\n",
    "    file = open(file_name, 'r')\n",
    "    ids = [l[0:-1] for l in file if os.path.exists(f\"/nfs/home/adatay20/votenet/scannet/scannet_train_detection_data/{l[0:-1]}_vert.npy\")]\n",
    "    data_dir = \"/nfs/home/adatay20/votenet/scannet/scannet_train_detection_data\"\n",
    "    data = ScanNetDetectionDataset(scannet_dir, ids, voxel_size, transform)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5247d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    coords_list = []\n",
    "    feats_list = []\n",
    "    xyz_list = []\n",
    "    bboxes_list = []\n",
    "\n",
    "    for i, (coords, feats, xyz, bboxes) in enumerate(batch):\n",
    "        coords_list.append(coords)\n",
    "        feats_list.append(feats)\n",
    "        \n",
    "        N = xyz.shape[1]\n",
    "        if N < 50000:\n",
    "            repeat_idx = np.random.choice(N, 50000 - N)\n",
    "            repeated_points = xyz[:, repeat_idx, :]\n",
    "            padded_points = torch.cat([xyz, repeated_points], axis=1)\n",
    "            xyz_list.append(padded_points)\n",
    "        else:\n",
    "            xyz_list.append(xyz)     # just append the dicts — they remain per-sample\n",
    "        bboxes_list.append(bboxes)    # variable-size tensors, stay in list\n",
    "    batched_xyz = torch.cat(xyz_list, dim=0)\n",
    "    xyz_dict = {'point_clouds': batched_xyz}\n",
    "\n",
    "    return feats_list, coords_list, xyz_dict, bboxes_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c1fc9",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66899c3-d9b7-40d9-aaba-47ace73ab0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points(points, percentage):\n",
    "    num_samples = int(np.random.uniform(percentage, 1.) * points.shape[0])\n",
    "    point_range = range(len(points))\n",
    "    choices = np.random.choice(point_range, num_samples, replace=False)\n",
    "    return points[choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d7c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_batch_dim(ft, ct):\n",
    "    batches = ct[:,0]\n",
    "    batch_size = batches.max().item() + 1\n",
    "    #nr_features = ft.shape[0] // batch_size\n",
    "    nr_features = torch.bincount(batches)\n",
    "    max_nr_features = torch.max(nr_features)\n",
    "    return_coords = torch.zeros([batch_size, max_nr_features , 3], device=ct.device)\n",
    "    return_features = torch.zeros([batch_size, 128, max_nr_features], device=ft.device)\n",
    "    for b in range(batch_size):\n",
    "        mask = batches == b\n",
    "        selected_feats = ft[mask]\n",
    "        selected_coords = ct[mask][:, 1:]\n",
    "        \n",
    "        return_features[b, :, :selected_feats.shape[0]] = selected_feats.T\n",
    "        return_coords[b, :selected_coords.shape[0], :] = selected_coords\n",
    "    return return_features, return_coords, nr_features\n",
    "\n",
    "def to_batch_output(ct, bb, cl, po):\n",
    "    batches = ct[:,0]\n",
    "    batch_size = batches.max().item() + 1\n",
    "    \n",
    "    out_bb, out_cl, out_po = [], [], []\n",
    "    for b in range(batch_size):\n",
    "        mask = batches == b\n",
    "        bb_batch = bb[mask]\n",
    "        cl_batch = cl[mask]\n",
    "        po_batch = po[mask]\n",
    "        \n",
    "        out_bb.append(bb_batch)\n",
    "        out_cl.append(cl_batch)\n",
    "        out_po.append(po_batch)\n",
    "    return [out_bb], [out_cl], [out_po]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b91fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse_dim(ct, bt):\n",
    "    B, C, N, _ = bt.shape\n",
    "    device = bt.device\n",
    "    batch_ids = ct[:, 0].long()  # [B*N]\n",
    "\n",
    "    sorted_ids, sorted_idx = torch.sort(batch_ids)\n",
    "    counts = torch.bincount(batch_ids, minlength=B)\n",
    "\n",
    "    point_indices_sorted = torch.cat([torch.arange(c, device=device) for c in counts.tolist()])\n",
    "    point_indices = torch.empty_like(point_indices_sorted)\n",
    "    point_indices[sorted_idx] = point_indices_sorted\n",
    "    out = bt[batch_ids, :, point_indices, 0]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c41a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label_list):\n",
    "    class_map = {\n",
    "        3 : 0,\n",
    "        4 : 1,\n",
    "        5 : 2,\n",
    "        6 : 3,\n",
    "        7 : 4,\n",
    "        8 : 5,\n",
    "        9 : 6,\n",
    "        10 : 7,\n",
    "        11 : 8,\n",
    "        12 : 9,\n",
    "        14 : 10,\n",
    "        16 : 11,\n",
    "        24 : 12,\n",
    "        28 : 13,\n",
    "        33 : 14,\n",
    "        34 : 15,\n",
    "        36 : 16,\n",
    "        39 : 17\n",
    "    }\n",
    "    return_tensor = torch.zeros([label_list.shape[0], 18], device = device)\n",
    "    for i, l in enumerate(label_list):\n",
    "        return_tensor[i, class_map[int(l)]] = 1\n",
    "    #print(return_tensor)\n",
    "    return return_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365bcbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tensor, new_min=0.0, new_max=1.0):\n",
    "    t_min, t_max = tensor.min(), tensor.max()\n",
    "    if t_min == t_max:\n",
    "        return torch.full_like(tensor, new_min)\n",
    "    return new_min + (tensor - t_min) * (new_max - new_min) / (t_max - t_min)\n",
    "\n",
    "def normalize_points_color(points, color_mean=None):\n",
    "    \"\"\"\n",
    "    Normalize RGB color channels in point cloud.\n",
    "    \n",
    "    Args:\n",
    "        points (np.ndarray): (N, 6) array, with columns [x, y, z, r, g, b]\n",
    "        color_mean (list or None): Mean values for r, g, b channels. \n",
    "                                   If None, compute from data.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Normalized points with shape (N, 6)\n",
    "    \"\"\"\n",
    "    assert points.shape[1] >= 6, \"Expected points with at least 6 dimensions (x, y, z, r, g, b)\"\n",
    "    \n",
    "    colors = points[:, 3:6].astype(np.float32)\n",
    "    \n",
    "    if color_mean is None:\n",
    "        color_mean = colors.mean(axis=0)\n",
    "    \n",
    "    # Normalize RGB values: (value - mean) / 255.0\n",
    "    normalized_colors = (colors - color_mean) / 255.0\n",
    "    \n",
    "    # Replace original colors with normalized ones\n",
    "    points[:, 3:6] = normalized_colors\n",
    "    return points\n",
    "\n",
    "def to_real_coords(voxels, voxel_size):\n",
    "    #norm_voxels = []\n",
    "    \n",
    "    \"\"\"for p, v in zip(points, voxels):\n",
    "        p_min = p.min()\n",
    "        p_max = p.max()\n",
    "        #print(\"voxels:\", v.min(), v.max())\n",
    "        #print(\"points:\", p_min, p_max)\n",
    "        \n",
    "        #norm_voxels.append(normalize(v, p_min, p_max))\n",
    "    \"\"\"\n",
    "        \n",
    "    return (voxels.float()) * voxel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab3e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_feat(vote_coords, vote_feat, tr3d_coords, tr3d_feat, lengths):\n",
    "    #print(\"Vote coords\", vote_coords.shape)\n",
    "    #print(\"TR3D coords\", tr3d_coords.shape)\n",
    "    _, ids, _ = knn_points(tr3d_coords[:,:,0:3], vote_coords[:,:,0:3],K=1, lengths1=lengths)\n",
    "    index = ids.squeeze(-1).unsqueeze(1)  # remove last dim, add dim for channels\n",
    "    index = index.expand(-1, 256, -1)\n",
    "    output = torch.gather(vote_feat, dim=2, index=index).to(device)\n",
    "    #print(\"MATCHED:\", output.shape)\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a32ed4",
   "metadata": {},
   "source": [
    "# RGBD Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27632ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TR3D.mink_resnet_TR3D import TR3DMinkResNet\n",
    "from TR3D.tr3d_neck import TR3DNeck\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RGBDBackbone(nn.Module):\n",
    "    def __init__(self, weights_path=\"TR3D.pth\"):\n",
    "        super().__init__()\n",
    "        self.in_channels = 3\n",
    "        self.depth = 34\n",
    "        self.norm = 'batch'\n",
    "        self.num_planes=(64, 128, 128, 128)\n",
    "        self.backbone = TR3DMinkResNet(in_channels=self.in_channels, depth=self.depth, \n",
    "                                    norm=self.norm, num_planes=self.num_planes, pool=False)\n",
    "        \n",
    "        self.neck_in_channels=(64, 128, 128, 128) \n",
    "        self.neck_out_channels=128\n",
    "        self.neck = TR3DNeck(in_channels=self.neck_in_channels, out_channels=self.neck_out_channels)\n",
    "        \n",
    "        # initialize weights of backbone and neck\n",
    "        if os.path.exists(weights_path):\n",
    "            state_dict = torch.load(weights_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), weights_only=False)\n",
    "            backbone_state = {k.replace('backbone.', ''): v for k, v in state_dict['state_dict'].items() if k.startswith('backbone.')}\n",
    "            neck_state = {k.replace('neck.', ''): v for k, v in state_dict['state_dict'].items() if k.startswith('neck.')}\n",
    "            self.neck.load_state_dict(neck_state, strict=True)\n",
    "            self.backbone.load_state_dict(backbone_state, strict=True)\n",
    "            print(f\"Loaded pretrained weights from {weights_path}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Pretrained weights not found at {weights_path}\")\n",
    "\n",
    "    def forward(self,input_rgb):\n",
    "        features = self.backbone(input_rgb)\n",
    "        modified_features = self.neck(features)\n",
    "        return modified_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfef9dd",
   "metadata": {},
   "source": [
    "# LiDAR Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from votenet.models.backbone_module import Pointnet2Backbone\n",
    "from votenet.models.voting_module import VotingModule\n",
    "from votenet.models.proposal_module import ProposalModule\n",
    "from votenet.models.dump_helper import dump_results\n",
    "from votenet.models.loss_helper import get_loss\n",
    "\n",
    "\n",
    "class LiDARBackbone(nn.Module):\n",
    "    r\"\"\"\n",
    "        A deep neural network for 3D object detection with end-to-end optimizable hough voting.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_class: int\n",
    "            Number of semantics classes to predict over -- size of softmax classifier\n",
    "        num_heading_bin: int\n",
    "        num_size_cluster: int\n",
    "        input_feature_dim: (default: 0)\n",
    "            Input dim in the feature descriptor for each point.  If the point cloud is Nx9, this\n",
    "            value should be 6 as in an Nx9 point cloud, 3 of the channels are xyz, and 6 are feature descriptors\n",
    "        num_proposal: int (default: 128)\n",
    "            Number of proposals/detections generated from the network. Each proposal is a 3D OBB with a semantic class.\n",
    "        vote_factor: (default: 1)\n",
    "            Number of votes generated from each seed point.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class, num_heading_bin, num_size_cluster, mean_size_arr,\n",
    "        input_feature_dim=1, num_proposal=128, vote_factor=1, sampling='vote_fps', backbone_path='votenet_backbone.pth',neck_path='votenet_neck.pth'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_class = num_class\n",
    "        self.num_heading_bin = num_heading_bin\n",
    "        self.num_size_cluster = num_size_cluster\n",
    "        self.mean_size_arr = mean_size_arr\n",
    "        assert(mean_size_arr.shape[0] == self.num_size_cluster)\n",
    "        self.input_feature_dim = input_feature_dim\n",
    "        self.num_proposal = num_proposal\n",
    "        self.vote_factor = vote_factor\n",
    "        self.sampling=sampling\n",
    "\n",
    "        # Backbone point feature learning\n",
    "        self.backbone_net = Pointnet2Backbone(input_feature_dim=self.input_feature_dim)\n",
    "\n",
    "        # Hough voting (Neck)\n",
    "        self.vgen = VotingModule(self.vote_factor, 256)\n",
    "            \n",
    "        # initialize weights of backbone and neck\n",
    "        if os.path.exists(neck_path) and os.path.exists(backbone_path):\n",
    "            neck_state = torch.load(neck_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), weights_only=False)\n",
    "            backbone_state = torch.load(backbone_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), weights_only=False)\n",
    "            self.backbone_net.load_state_dict(backbone_state, strict=True)\n",
    "            self.vgen.load_state_dict(neck_state, strict=True)\n",
    "            print(f\"Loaded pretrained weights from {backbone_path} and {neck_path}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Pretrained weights not found at {backbone_path} or {neck_path}\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Forward pass of the network\n",
    "\n",
    "        Args:\n",
    "            inputs: dict\n",
    "                {point_clouds}\n",
    "\n",
    "                point_clouds: Variable(torch.cuda.FloatTensor)\n",
    "                    (B, N, 3 + input_channels) tensor\n",
    "                    Point cloud to run predicts on\n",
    "                    Each point in the point-cloud MUST\n",
    "                    be formated as (x, y, z, features...)\n",
    "        Returns:\n",
    "            end_points: dict\n",
    "        \"\"\"\n",
    "        \n",
    "        end_points = {}\n",
    "        batch_size = inputs['point_clouds'].shape[0]\n",
    "\n",
    "        end_points = self.backbone_net(inputs['point_clouds'], end_points)\n",
    "                \n",
    "        # --------- HOUGH VOTING ---------\n",
    "        xyz = end_points['fp2_xyz']\n",
    "        features = end_points['fp2_features']\n",
    "        end_points['seed_inds'] = end_points['fp2_inds']\n",
    "        end_points['seed_xyz'] = xyz\n",
    "        end_points['seed_features'] = features\n",
    "        \n",
    "        xyz, features = self.vgen(xyz, features)\n",
    "        features_norm = torch.norm(features, p=2, dim=1)\n",
    "        features = features.div(features_norm.unsqueeze(1))\n",
    "        end_points['vote_xyz'] = xyz\n",
    "        end_points['vote_features'] = features\n",
    "\n",
    "        return end_points['vote_features'], end_points['vote_xyz']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eca682",
   "metadata": {},
   "source": [
    "# Fusion Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e980f",
   "metadata": {},
   "source": [
    "# Detection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3280952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TR3DHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_reg_outs, num_classes, voxel_size = 0.02, pts_center_threshold=10, weights_path=\"TR3D.pth\"):\n",
    "        super().__init__()\n",
    "        self.voxel_size = voxel_size\n",
    "        self.pts_center_threshold = pts_center_threshold\n",
    "\n",
    "        self.conv_reg = ME.MinkowskiConvolution(\n",
    "            in_channels, num_reg_outs, kernel_size=1, bias=True, dimension=3\n",
    "        )\n",
    "        self.conv_cls = ME.MinkowskiConvolution(\n",
    "            in_channels, num_classes, kernel_size=1, bias=True, dimension=3\n",
    "        )\n",
    "\n",
    "        # Init\n",
    "        nn.init.normal_(self.conv_reg.kernel, std=0.01)\n",
    "        nn.init.normal_(self.conv_cls.kernel, std=0.01)\n",
    "        nn.init.constant_(self.conv_cls.bias, -torch.log(torch.tensor((1 - 0.01) / 0.01)))\n",
    "        \n",
    "    def forward_single(self, x):\n",
    "        reg_out = self.conv_reg(x).features\n",
    "        cls_out = self.conv_cls(x).features\n",
    "\n",
    "        reg_distance = torch.exp(reg_out[:, 3:6])\n",
    "        reg_angle = reg_out[:, 6:] if reg_out.shape[1] > 6 else None\n",
    "        bbox_pred = torch.cat([reg_out[:, :3], reg_distance, reg_angle], dim=1) if reg_angle is not None else torch.cat([reg_out[:, :3], reg_distance], dim=1)\n",
    "\n",
    "        return bbox_pred, cls_out, x.coordinates[:, 1:] * self.voxel_size\n",
    "\n",
    "    def forward(self, sparse_tensor_list):\n",
    "        all_bbox_preds, all_cls_preds, all_points = [], [], []\n",
    "        for x in sparse_tensor_list:\n",
    "            bbox_pred, cls_pred, points = self.forward_single(x)\n",
    "            all_bbox_preds.append(bbox_pred)\n",
    "            all_cls_preds.append(cls_pred)\n",
    "            all_points.append(points)\n",
    "        return all_bbox_preds, all_cls_preds, all_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae897733",
   "metadata": {},
   "source": [
    "# Loss Function / Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TR3D.IoU_TR3D_loss import TR3DAxisAlignedIoULoss\n",
    "from TR3D.Focal_loss import FocalLoss\n",
    "from typing import List, Optional, Tuple\n",
    "from torch import Tensor\n",
    "from TR3D.iou_3d import calculate_map\n",
    "from TR3D.IoU_TR3D_loss import AxisAlignedBboxOverlaps3D\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_map(pred_boxes, gt_boxes, pred_cls, gt_labels, iou_threshold=0.25, num_classes=18):\n",
    "    \"\"\"\n",
    "    Compute mean Average Precision (mAP) for 3D object detection.\n",
    "    \n",
    "    Parameters:\n",
    "        pred_boxes: List of predicted boxes [x, y, z, dx, dy, dz, class_id, confidence]\n",
    "        gt_boxes:   List of ground truth boxes [x, y, z, dx, dy, dz, class_id]\n",
    "        iou_threshold: IoU threshold for matching\n",
    "        num_classes: number of classes\n",
    "        \n",
    "    Returns:\n",
    "        mAP: Mean Average Precision across classes\n",
    "        ap_per_class: Dictionary of AP per class\n",
    "    \"\"\"\n",
    "    gt_by_class = defaultdict(list)\n",
    "    pred_by_class = defaultdict(list)\n",
    "\n",
    "    for gt, l in zip(gt_boxes, gt_labels):\n",
    "        gt_by_class[int(l)].append(gt[:6])\n",
    "    for pred, l in zip(pred_boxes, pred_cls):\n",
    "        pred_by_class[int(torch.argmax(l))].append(pred[:6])  # (box, confidence)\n",
    "\n",
    "    ap_per_class = {}\n",
    "    eps = 1e-6\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        gts = gt_by_class[cls]\n",
    "        preds = pred_by_class[cls]\n",
    "\n",
    "        if len(gts) == 0 and len(preds) == 0:\n",
    "            ap_per_class[cls] = None  # no predictions or targets — perfect\n",
    "            continue\n",
    "        elif len(gts) == 0:\n",
    "            ap_per_class[cls] = 0.0\n",
    "            continue\n",
    "\n",
    "        # Sort predictions by confidence\n",
    "        preds = sorted(preds, key=lambda x: -x[1])\n",
    "        tp = np.zeros(len(preds))\n",
    "        fp = np.zeros(len(preds))\n",
    "        matched_gt = set()\n",
    "        for i, pred_box in enumerate(preds):\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "            for j, gt_box in enumerate(gts):\n",
    "                if j in matched_gt:\n",
    "                    continue\n",
    "                if len(pred_box.shape) != 3:\n",
    "                    pred_box = pred_box.unsqueeze(0).unsqueeze(0)\n",
    "                if len(gt_box.shape) != 3:\n",
    "                    gt_box = gt_box.unsqueeze(0).unsqueeze(0)\n",
    "                iou = AxisAlignedBboxOverlaps3D()(LossFunction._bbox_to_loss(pred_box), LossFunction._bbox_to_loss(gt_box))\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = j\n",
    "\n",
    "            if best_iou >= iou_threshold:\n",
    "                tp[i] = 1\n",
    "                matched_gt.add(best_gt_idx)\n",
    "            else:\n",
    "                fp[i] = 1\n",
    "\n",
    "        tp_cumsum = np.cumsum(tp)\n",
    "        fp_cumsum = np.cumsum(fp)\n",
    "        recalls = tp_cumsum / (len(gts) + eps)\n",
    "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + eps)\n",
    "\n",
    "        ap = 0.0\n",
    "        for t in np.linspace(0, 1, 101):\n",
    "            if np.sum(recalls >= t) == 0:\n",
    "                p = 0\n",
    "            else:\n",
    "                p = np.max(precisions[recalls >= t])\n",
    "            ap += p / 101\n",
    "        ap_per_class[cls] = ap\n",
    "\n",
    "    values = [v for v in ap_per_class.values() if v is not None]\n",
    "    mAP = np.mean(values)\n",
    "    return mAP, ap_per_class\n",
    "\n",
    "class LossFunction(nn.Module):\n",
    "    def __init__(self, weights = None):\n",
    "        super().__init__()\n",
    "        self.bbox_loss = TR3DAxisAlignedIoULoss(mode='diou', reduction='none')\n",
    "        self.cls_loss = FocalLoss(gamma=2.0, alpha=0.25, reduction = \"none\")\n",
    "        self.label2level = [0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0]\n",
    "        self.pts_center_threshold = 6\n",
    "        self.weights = weights\n",
    "    \n",
    "    def _loss_by_feat_single(self, bbox_preds: List[Tensor],\n",
    "                         cls_preds: List[Tensor], points: List[Tensor],\n",
    "                         gt_bboxes: Tensor, gt_labels: Tensor) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"Loss function of single sample.\n",
    "\n",
    "        Args:\n",
    "            bbox_preds (list[Tensor]): Bbox predictions for all levels.\n",
    "            cls_preds (list[Tensor]): Classification predictions for all\n",
    "                levels.\n",
    "            points (list[Tensor]): Final location coordinates for all levels.\n",
    "            gt_bboxes (:obj:`BaseInstance3DBoxes`): Ground truth boxes.\n",
    "            gt_labels (Tensor): Ground truth labels.\n",
    "            input_meta (dict): Scene meta info.\n",
    "\n",
    "        Returns:\n",
    "            tuple[Tensor, ...]: Bbox and classification loss\n",
    "                values and a boolean mask of assigned points.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_classes = cls_preds[0].shape[1]\n",
    "        bbox_targets, cls_targets = self.get_targets(points, gt_bboxes,\n",
    "                                                     gt_labels, num_classes)\n",
    "        bbox_preds_cat = torch.cat(bbox_preds)\n",
    "        cls_preds_cat = torch.cat(cls_preds)\n",
    "        points = torch.cat(points)\n",
    "\n",
    "        \n",
    "        # cls loss\n",
    "        num_classes = cls_preds_cat.size(1)\n",
    "        cls_loss = self.cls_loss(cls_preds_cat, cls_targets, self.weights[cls_targets])\n",
    "\n",
    "        \n",
    "        # bbox loss\n",
    "        pos_mask = cls_targets < num_classes\n",
    "        pos_bbox_preds = bbox_preds_cat[pos_mask]\n",
    "        \n",
    "        \n",
    "        if pos_mask.sum() > 0:\n",
    "            pos_points = points[pos_mask]\n",
    "            pos_bbox_preds = bbox_preds_cat[pos_mask]\n",
    "            pos_bbox_targets = bbox_targets[pos_mask]\n",
    "            pos_pred_cls = cls_preds_cat[pos_mask]\n",
    "            pos_target_cls = cls_targets[pos_mask]\n",
    "            \n",
    "            size_loss = (pos_bbox_preds[..., 3:] - 5.0).clamp(min=0) ** 2\n",
    "            \n",
    "            transformed_pred_bbox = self._bbox_to_loss(self._bbox_pred_to_bbox(pos_points, pos_bbox_preds))\n",
    "            transformed_bbox_targets = self._bbox_to_loss(pos_bbox_targets)\n",
    "            \n",
    "            bbox_loss = self.bbox_loss(transformed_pred_bbox, transformed_bbox_targets)\n",
    "            cls_thresholds = torch.max(torch.sigmoid(pos_pred_cls), dim=1, keepdim=True)[0][:,0]\n",
    "            cls_ret = cls_thresholds.max()\n",
    "            mAP, AP = calculate_map(self._bbox_pred_to_bbox(pos_points, pos_bbox_preds)[cls_thresholds > 0.05], gt_bboxes,\n",
    "                                  pos_pred_cls[cls_thresholds > 0.05], gt_labels)\n",
    "            \n",
    "        else:\n",
    "            cls_ret = 0\n",
    "            mAP = 0\n",
    "            AP = {i:0 for i in range(18)}\n",
    "            size_loss = torch.zeros_like(pos_bbox_preds[...,3:])\n",
    "            bbox_loss = pos_bbox_preds\n",
    "        return bbox_loss, cls_loss, pos_mask, size_loss, mAP, AP, cls_ret\n",
    "\n",
    "    def loss_by_feat(self,\n",
    "                 bbox_preds: List[List[Tensor]],\n",
    "                 cls_preds: List[List[Tensor]],\n",
    "                 points: List[List[Tensor]],\n",
    "                 batch_gt_bboxes_3d: List[Tensor],\n",
    "                 batch_gt_instances_ignore = None) -> dict:\n",
    "        \"\"\"Loss function about feature.\n",
    "\n",
    "        Args:\n",
    "            bbox_preds (list[list[Tensor]]): Bbox predictions for all scenes.\n",
    "                The first list contains predictions from different\n",
    "                levels. The second list contains predictions in a mini-batch.\n",
    "            cls_preds (list[list[Tensor]]): Classification predictions for all\n",
    "                scenes. The first list contains predictions from different\n",
    "                levels. The second list contains predictions in a mini-batch.\n",
    "            points (list[list[Tensor]]): Final location coordinates for all\n",
    "                scenes. The first list contains predictions from different\n",
    "                levels. The second list contains predictions in a mini-batch.\n",
    "            batch_gt_instances_3d (list[:obj:`InstanceData`]): Batch of\n",
    "                gt_instance_3d.  It usually includes ``bboxes_3d``、`\n",
    "                `labels_3d``、``depths``、``centers_2d`` and attributes.\n",
    "            batch_input_metas (list[dict]): Meta information of each image,\n",
    "                e.g., image size, scaling factor, etc.\n",
    "\n",
    "        Returns:\n",
    "            dict: Bbox, and classification losses.\n",
    "        \"\"\"\n",
    "        bbox_losses, cls_losses, pos_masks, size_losses, mAPs, APs, ths = [], [], [], [], [], [], []\n",
    "        for i in range(len(batch_gt_bboxes_3d)):\n",
    "            bbox_loss, cls_loss, pos_mask, size_loss, mAP, AP, cls_th = self._loss_by_feat_single(\n",
    "                bbox_preds=[x[i] for x in bbox_preds],\n",
    "                cls_preds=[x[i] for x in cls_preds],\n",
    "                points=[x[i][:,0:3] for x in points],\n",
    "                gt_bboxes=batch_gt_bboxes_3d[i][:, 0:6],\n",
    "                gt_labels=batch_gt_bboxes_3d[i][:,6].long())\n",
    "            if len(bbox_loss) > 0:\n",
    "                bbox_losses.append(bbox_loss)\n",
    "            cls_losses.append(cls_loss)\n",
    "            pos_masks.append(pos_mask)\n",
    "            size_losses.append(size_loss)\n",
    "            mAPs.append(mAP)\n",
    "            APs.append(AP)\n",
    "            ths.append(cls_th)\n",
    "        #print(max(ths))\n",
    "        return dict(\n",
    "            bbox_loss=torch.mean(torch.cat(bbox_losses)),\n",
    "            cls_loss=torch.sum(torch.cat(cls_losses)) / \n",
    "            torch.sum(torch.cat(pos_masks)),\n",
    "            size_loss=torch.mean(torch.cat(size_losses))), np.mean(mAPs), APs\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _bbox_to_loss(bbox):\n",
    "        \"\"\"Transform box to the axis-aligned or rotated iou loss format.\n",
    "\n",
    "        Args:\n",
    "            bbox (Tensor): 3D box of shape (N, 6) or (N, 7).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Transformed 3D box of shape (N, 6) or (N, 7).\n",
    "        \"\"\"\n",
    "        # rotated iou loss accepts (x, y, z, w, h, l, heading)\n",
    "        if bbox.shape[-1] != 6:\n",
    "            return bbox\n",
    "\n",
    "        # axis-aligned case: x, y, z, w, h, l -> x1, y1, z1, x2, y2, z2\n",
    "        return torch.stack(\n",
    "            (bbox[..., 0] - bbox[..., 3] / 2, bbox[..., 1] - bbox[..., 4] / 2, bbox[..., 2] - bbox[..., 5] / 2, \n",
    "             bbox[..., 0] + bbox[..., 3] / 2, bbox[..., 1] + bbox[..., 4] / 2, bbox[..., 2] + bbox[..., 5] / 2),\n",
    "            dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _bbox_pred_to_bbox(points, bbox_pred):\n",
    "        \"\"\"Transform predicted bbox parameters to bbox.\n",
    "\n",
    "        Args:\n",
    "            points (Tensor): Final locations of shape (N, 3)\n",
    "            bbox_pred (Tensor): Predicted bbox parameters of shape (N, 6)\n",
    "                or (N, 8).\n",
    "        Returns:\n",
    "            Tensor: Transformed 3D box of shape (N, 6) or (N, 7).\n",
    "        \"\"\"\n",
    "        if bbox_pred.shape[0] == 0:\n",
    "            return bbox_pred\n",
    "\n",
    "        x_center = points[:, 0] + bbox_pred[:, 0]\n",
    "        y_center = points[:, 1] + bbox_pred[:, 1]\n",
    "        z_center = points[:, 2] + bbox_pred[:, 2]\n",
    "        base_bbox = torch.stack([\n",
    "            x_center, y_center, z_center, bbox_pred[:, 3], bbox_pred[:, 4],\n",
    "            bbox_pred[:, 5]\n",
    "        ], -1)\n",
    "\n",
    "        # axis-aligned case\n",
    "        if bbox_pred.shape[1] == 6:\n",
    "            return base_bbox\n",
    "\n",
    "        # rotated case: ..., sin(2a)ln(q), cos(2a)ln(q)\n",
    "        scale = bbox_pred[:, 3] + bbox_pred[:, 4]\n",
    "        q = torch.exp(\n",
    "            torch.sqrt(\n",
    "                torch.pow(bbox_pred[:, 6], 2) + torch.pow(bbox_pred[:, 7], 2)))\n",
    "        alpha = 0.5 * torch.atan2(bbox_pred[:, 6], bbox_pred[:, 7])\n",
    "        return torch.stack(\n",
    "            (x_center, y_center, z_center, scale / (1 + q), scale /\n",
    "             (1 + q) * q, bbox_pred[:, 5] + bbox_pred[:, 4], alpha),\n",
    "            dim=-1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_targets(self, points: Tensor, gt_bboxes: Tensor,\n",
    "                    gt_labels: Tensor, num_classes: int) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"Compute targets for final locations for a single scene.\n",
    "\n",
    "        Args:\n",
    "            points (list[Tensor]): Final locations for all levels.\n",
    "            gt_bboxes (BaseInstance3DBoxes): Ground truth boxes.\n",
    "            gt_labels (Tensor): Ground truth labels.\n",
    "            num_classes (int): Number of classes.\n",
    "\n",
    "        Returns:\n",
    "            tuple[Tensor, ...]: Bbox and classification targets for all\n",
    "                locations.\n",
    "        \"\"\"\n",
    "        float_max = points[0].new_tensor(1e8)\n",
    "        levels = torch.cat([\n",
    "            points[i].new_tensor(i, dtype=torch.long).expand(len(points[i]))\n",
    "            for i in range(len(points))\n",
    "        ])\n",
    "        points = torch.cat(points)\n",
    "        n_points = len(points)\n",
    "        n_boxes = len(gt_bboxes)\n",
    "\n",
    "        if len(gt_labels) == 0:\n",
    "            return points.new_tensor([]), \\\n",
    "                gt_labels.new_full((n_points,), num_classes)\n",
    "        \n",
    "        zero_col = torch.zeros([n_boxes,1], device=gt_bboxes.device)\n",
    "        boxes = torch.cat([gt_bboxes, zero_col], dim=1)\n",
    "        #print(\"box\", boxes.shape)\n",
    "        boxes = boxes.expand(n_points, n_boxes, 7)\n",
    "        points = points.unsqueeze(1).expand(n_points, n_boxes, 3)\n",
    "\n",
    "        # condition 1: fix level for label\n",
    "        label2level = gt_labels.new_tensor(self.label2level)\n",
    "        label_levels = label2level[gt_labels].unsqueeze(0).expand(\n",
    "            n_points, n_boxes)\n",
    "        point_levels = torch.unsqueeze(levels, 1).expand(n_points, n_boxes)\n",
    "        level_condition = label_levels == point_levels\n",
    "\n",
    "        # condition 2: keep topk location per box by center distance\n",
    "        center = boxes[..., :3]\n",
    "        center_distances = torch.sum(torch.pow(center - points, 2), dim=-1)\n",
    "        center_distances = torch.where(level_condition, center_distances,\n",
    "                                       float_max)\n",
    "        topk_distances = torch.topk(\n",
    "            center_distances,\n",
    "            min(self.pts_center_threshold + 1, len(center_distances)),\n",
    "            largest=False,\n",
    "            dim=0).values[-1]\n",
    "        topk_condition = center_distances < topk_distances.unsqueeze(0)\n",
    "\n",
    "        # condition 3: min center distance to box per point\n",
    "        center_distances = torch.where(topk_condition, center_distances,\n",
    "                                       float_max)\n",
    "        min_values, min_ids = center_distances.min(dim=1)\n",
    "        min_inds = torch.where(min_values < float_max, min_ids, -1)\n",
    "\n",
    "        bbox_targets = boxes[0][min_inds]\n",
    "        \n",
    "        bbox_targets = bbox_targets[:, :-1]\n",
    "        cls_targets = torch.where(min_inds >= 0, gt_labels[min_inds],\n",
    "                                  num_classes)\n",
    "        return bbox_targets, cls_targets\n",
    "\n",
    "    def _single_scene_multiclass_nms(self, bboxes: Tensor, scores: Tensor,\n",
    "                                     input_meta: dict) -> Tuple[Tensor, ...]:\n",
    "        \"\"\"Multi-class nms for a single scene.\n",
    "\n",
    "        Args:\n",
    "            bboxes (Tensor): Predicted boxes of shape (N_boxes, 6) or\n",
    "                (N_boxes, 7).\n",
    "            scores (Tensor): Predicted scores of shape (N_boxes, N_classes).\n",
    "            input_meta (dict): Scene meta data.\n",
    "\n",
    "        Returns:\n",
    "            tuple[Tensor, ...]: Predicted bboxes, scores and labels.\n",
    "        \"\"\"\n",
    "        num_classes = scores.shape[1]\n",
    "        with_yaw = bboxes.shape[1] == 7\n",
    "        nms_bboxes, nms_scores, nms_labels = [], [], []\n",
    "        for i in range(num_classes):\n",
    "            ids = scores[:, i] > self.test_cfg.score_thr\n",
    "            if not ids.any():\n",
    "                continue\n",
    "\n",
    "            class_scores = scores[ids, i]\n",
    "            class_bboxes = bboxes[ids]\n",
    "            if with_yaw:\n",
    "                nms_function = nms3d\n",
    "            else:\n",
    "                class_bboxes = torch.cat(\n",
    "                    (class_bboxes, torch.zeros_like(class_bboxes[:, :1])),\n",
    "                    dim=1)\n",
    "                nms_function = nms3d_normal\n",
    "\n",
    "            nms_ids = nms_function(class_bboxes, class_scores,\n",
    "                                   self.test_cfg.iou_thr)\n",
    "            nms_bboxes.append(class_bboxes[nms_ids])\n",
    "            nms_scores.append(class_scores[nms_ids])\n",
    "            nms_labels.append(\n",
    "                bboxes.new_full(\n",
    "                    class_scores[nms_ids].shape, i, dtype=torch.long))\n",
    "\n",
    "        if len(nms_bboxes):\n",
    "            nms_bboxes = torch.cat(nms_bboxes, dim=0)\n",
    "            nms_scores = torch.cat(nms_scores, dim=0)\n",
    "            nms_labels = torch.cat(nms_labels, dim=0)\n",
    "        else:\n",
    "            nms_bboxes = bboxes.new_zeros((0, bboxes.shape[1]))\n",
    "            nms_scores = bboxes.new_zeros((0, ))\n",
    "            nms_labels = bboxes.new_zeros((0, ))\n",
    "\n",
    "        if not with_yaw:\n",
    "            nms_bboxes = nms_bboxes[:, :6]\n",
    "\n",
    "        return nms_bboxes, nms_scores, nms_labels\n",
    "    \n",
    "    def forward(self,\n",
    "                 bbox_preds: List[List[Tensor]],\n",
    "                 cls_preds: List[List[Tensor]],\n",
    "                 points: List[List[Tensor]],\n",
    "                 batch_gt_instances_3d: List[Tensor],\n",
    "                 batch_gt_instances_ignore = None):\n",
    "        return self.loss_by_feat(bbox_preds, cls_preds, points, batch_gt_instances_3d)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71bf666",
   "metadata": {},
   "source": [
    "# Full Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e2056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, rgb_backbone, lidar_backbone, voxel_size, n_classes=18, weights_path=\"TR3D.pth\"):\n",
    "        super().__init__()\n",
    "        self.voxel_size = voxel_size\n",
    "        self.rgb_backbone = rgb_backbone\n",
    "        self.lidar_backbone = lidar_backbone\n",
    "        \n",
    "        self.detection_head = TR3DHead(128 + 256, 6, 18)\n",
    "        \n",
    "        state_dict = torch.load(weights_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), weights_only=False)\n",
    "        head_state = {k.replace('head.', ''): v for k, v in state_dict['state_dict'].items() if k.startswith('head.')}\n",
    "        head_state.pop(\"conv_reg.kernel\", None)\n",
    "        head_state.pop(\"conv_cls.kernel\", None)\n",
    "        self.detection_head.load_state_dict(head_state, strict=False)\n",
    "        print(f\"Loaded pretrained weights from {weights_path}\")\n",
    "\n",
    "\n",
    "    def forward(self, rgb_input, lidar_input):\n",
    "        rgb_tensors = self.rgb_backbone(rgb_input)  # Dropout applied inside the RGBBackbone\n",
    "        \n",
    "        rgb_coords = [t.coordinates for t in rgb_tensors]\n",
    "        rgb_coords = torch.cat(rgb_coords)\n",
    "        \n",
    "        rgb_feats = [t.features for t in rgb_tensors]\n",
    "        rgb_feats = torch.cat(rgb_feats)\n",
    "        \n",
    "        lidar_features, lidar_coords = self.lidar_backbone(lidar_input)  # Dropout applied inside LiDARBackbone\n",
    "        \n",
    "        rgb_features, _, _ = to_batch_dim(rgb_feats, rgb_coords)\n",
    "        \n",
    "        pad_amount = rgb_features.size(2) - lidar_features.size(2)\n",
    "        \n",
    "        padded_lidar = F.pad(lidar_features, (0, pad_amount))\n",
    "        fused_feats = torch.cat([rgb_features, padded_lidar], dim=1)\n",
    "        fused_feats = fused_feats.unsqueeze(3)\n",
    "        \n",
    "        feats = to_sparse_dim(rgb_coords, fused_feats)\n",
    "        \n",
    "        \n",
    "             \n",
    "        sparse_fused = ME.SparseTensor(features=feats, coordinates=rgb_coords, device=device)\n",
    "        \n",
    "        bb, cl, po = self.detection_head([sparse_fused])\n",
    "        bb, cl, po = to_batch_output(sparse_fused.coordinates, \n",
    "                                     torch.cat(bb), torch.cat(cl), torch.cat(po))\n",
    "        return bb, cl, po"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9a767",
   "metadata": {},
   "source": [
    "# Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run():\n",
    "    mean_sizes=np.array([[0.76966727, 0.8116021, 0.92573744],\n",
    "                            [1.876858, 1.8425595, 1.1931566],\n",
    "                            [0.61328, 0.6148609, 0.7182701],\n",
    "                            [1.3955007, 1.5121545, 0.83443564],\n",
    "                            [0.97949594, 1.0675149, 0.6329687],\n",
    "                            [0.531663, 0.5955577, 1.7500148],\n",
    "                            [0.9624706, 0.72462326, 1.1481868],\n",
    "                            [0.83221924, 1.0490936, 1.6875663],\n",
    "                            [0.21132214, 0.4206159, 0.5372846],\n",
    "                            [1.4440073, 1.8970833, 0.26985747],\n",
    "                            [1.0294262, 1.4040797, 0.87554324],\n",
    "                            [1.3766412, 0.65521795, 1.6813129],\n",
    "                            [0.6650819, 0.71111923, 1.298853],\n",
    "                            [0.41999173, 0.37906948, 1.7513971],\n",
    "                            [0.59359556, 0.5912492, 0.73919016],\n",
    "                            [0.50867593, 0.50656086, 0.30136237],\n",
    "                            [1.1511526, 1.0546296, 0.49706793],\n",
    "                            [0.47535285, 0.49249494, 0.5802117]])\n",
    "\n",
    "    voxel_size = 0.2\n",
    "\n",
    "    votenet = LiDARBackbone(num_class=18, num_heading_bin=1, num_size_cluster=18, mean_size_arr = mean_sizes).to(device)\n",
    "\n",
    "    TR3D = RGBDBackbone().to(device)\n",
    "\n",
    "    test_fusion = CombinedModel(TR3D, votenet, voxel_size).to(device)\n",
    "\n",
    "    #bb, cl, po = test_fusion(test_dataset[0][0], test_dataset[0][1])\n",
    "    #print(bb[0][0], cl[0][0])\n",
    "\n",
    "    #--------------------------------\n",
    "\n",
    "    data = get_data('scannetv1_train.txt', TR3D, voxel_size)\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    #batch = next(iter(train_loader))\n",
    "    count = 0\n",
    "    count1 = 0\n",
    "\n",
    "    test_loss = LossFunction()\n",
    "    for feats, coords, votenet, bboxes_list in train_loader:\n",
    "        count1 += 1\n",
    "        #try:\n",
    "        bboxes = [bbox.to(device) for bbox in bboxes_list]\n",
    "        votenet['point_clouds'] = votenet['point_clouds'].to(device)\n",
    "\n",
    "        #print(\"Feats\", feats[0].shape)\n",
    "        coords, feats = ME.utils.sparse_collate(coords, feats)\n",
    "        #print(\"after sparse collate\", coords.shape, feats.shape)\n",
    "        sparse_tensor = ME.SparseTensor(features=feats.to(device), coordinates=coords.to(device),  # coordinates must be defined in a integer grid. If the scale\n",
    "        quantization_mode=ME.SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE)\n",
    "        bb, cl, po = test_fusion(sparse_tensor, votenet)\n",
    "        loss = test_loss(bb,cl,po,bboxes)\n",
    "        count += 1\n",
    "    print(count, count1)\n",
    "    \n",
    "#test_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40755f0c-88e8-4a02-b74f-e827ff7dbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_instances():\n",
    "    train_data = get_data('scannet_train.txt', 0.2)\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # change to >0 once it works stably\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    counts = [0] * 18\n",
    "    for feats, coords, lidar_inputs, bboxes_list in train_loader:\n",
    "        for bbox in bboxes_list[0]:\n",
    "            counts[int(bbox[6])] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "def get_class_weights(method='log_inv'):\n",
    "    \"\"\"\n",
    "    Returns normalized class weights as a torch.Tensor.\n",
    "    \n",
    "    Args:\n",
    "        class_counts (list or array-like): A list of the number of occurrences for each class.\n",
    "        method (str): Normalization method. Options are:\n",
    "                      'inverse' - inverse frequency (1 / count)\n",
    "                      'sqrt_inv' - inverse sqrt frequency (1 / sqrt(count))\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of normalized weights summing to 1.\n",
    "    \"\"\"\n",
    "    class_counts = count_instances()\n",
    "    counts = torch.tensor(class_counts, dtype=torch.float32, device=device)\n",
    "    if method == 'inverse':\n",
    "        weights = 1.0 / counts  # Avoid division by zero\n",
    "    elif method == 'sqrt_inv':\n",
    "        weights = 1.0 / torch.sqrt(counts)\n",
    "    elif method == \"log_inv\":\n",
    "        weights = torch.log(counts.sum() / counts)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported method. Use 'inverse' or 'sqrt_inv'.\")\n",
    "    # Normalize to sum to 1\n",
    "    #weights = weights / weights.sum()\n",
    "    print(weights)\n",
    "    zero_to_add = torch.tensor([2.0], device=device)  # must be a tensor with same dtype\n",
    "    return torch.cat((weights, zero_to_add))\n",
    "#print(count_instances())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f8b607",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c2783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_classes=18, num_epochs=3, batch_size=16, \n",
    "    learning_rate=0.001, debug=False, output_folder=\"output_fused_D\", class_names=None,\n",
    "    voxel_size = 0.2):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    mean_sizes=np.array([[0.76966727, 0.8116021, 0.92573744],\n",
    "                        [1.876858, 1.8425595, 1.1931566],\n",
    "                        [0.61328, 0.6148609, 0.7182701],\n",
    "                        [1.3955007, 1.5121545, 0.83443564],\n",
    "                        [0.97949594, 1.0675149, 0.6329687],\n",
    "                        [0.531663, 0.5955577, 1.7500148],\n",
    "                        [0.9624706, 0.72462326, 1.1481868],\n",
    "                        [0.83221924, 1.0490936, 1.6875663],\n",
    "                        [0.21132214, 0.4206159, 0.5372846],\n",
    "                        [1.4440073, 1.8970833, 0.26985747],\n",
    "                        [1.0294262, 1.4040797, 0.87554324],\n",
    "                        [1.3766412, 0.65521795, 1.6813129],\n",
    "                        [0.6650819, 0.71111923, 1.298853],\n",
    "                        [0.41999173, 0.37906948, 1.7513971],\n",
    "                        [0.59359556, 0.5912492, 0.73919016],\n",
    "                        [0.50867593, 0.50656086, 0.30136237],\n",
    "                        [1.1511526, 1.0546296, 0.49706793],\n",
    "                        [0.47535285, 0.49249494, 0.5802117]])\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize backbones\n",
    "    rgb_backbone = RGBDBackbone()  # Correctly instantiate the RGB backbone\n",
    "    lidar_backbone = LiDARBackbone(num_class=18, num_heading_bin=1, num_size_cluster=18, mean_size_arr = mean_sizes)  # Correctly instantiate the LiDAR backbone\n",
    "\n",
    "    # Multi-GPU setup\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "    model = CombinedModel(rgb_backbone, lidar_backbone, voxel_size)\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0, 1])  # Wrap model for multi-GPU usage\n",
    "    model = model.to(device)\n",
    "\n",
    "    def lidar_transform(lidar_data, gt_boxes):\n",
    "        if torch.rand(1).item() > 0.5:  # Apply random vertical flip with 50% chance\n",
    "            \n",
    "            lidar_data[:, 0] = -lidar_data[:, 0]\n",
    "            gt_boxes[:,0] = -gt_boxes[:,0]\n",
    "        if torch.rand(1).item() > 0.5:  # Apply random horizontal flip with 50% chance\n",
    "            lidar_data[:, 1] = -lidar_data[:, 1]\n",
    "            gt_boxes[:,1] = -gt_boxes[:,1]\n",
    "        return lidar_data, gt_boxes\n",
    "\n",
    "    print(f\"Using GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "    train_data = get_data('train.txt', voxel_size, lidar_transform)\n",
    "    val_data = get_data('val.txt', voxel_size)\n",
    "    test_data = get_data('test.txt', voxel_size)\n",
    "    \n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,  # change to >0 once it works stably\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    #train_dataset.lidar_transform = lidar_transform  # Attach lidar_transform to train_dataset\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,  # change to >0 once it works stably\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,  # change to >0 once it works stably\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    model = CombinedModel(rgb_backbone, lidar_backbone, voxel_size).to(device)\n",
    "    \n",
    "    for param in model.rgb_backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.rgb_backbone.eval()\n",
    "\n",
    "    # Freeze LiDAR backbone\n",
    "    for param in model.lidar_backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.lidar_backbone.eval()\n",
    "\n",
    "    weights = get_class_weights()\n",
    "    criterion = LossFunction(weights)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3, threshold=0.001)\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    metrics_path = os.path.join(output_folder, \"metrics.csv\")\n",
    "\n",
    "    log_dir = os.path.join(output_folder, \"tensorboard_logs\")\n",
    "    \n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    dummy_rgb = torch.randn(batch_size*370, 4).to(device)\n",
    "    dummy_lidar = torch.randn(1, 50000, 3).to(device)\n",
    "\n",
    "    #with torch.no_grad():\n",
    "        #writer.add_graph(model, (dummy_rgb, dummy_lidar))\n",
    "\n",
    "    columns = [\n",
    "        'epoch', 'train_loss', 'train_accuracy', 'train_f1',\n",
    "        'val_loss', 'val_accuracy', 'val_f1',\n",
    "        'test_loss', 'test_accuracy', 'test_f1'\n",
    "    ]\n",
    "    metrics_df = pd.DataFrame(columns=columns)\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    mean_APs = {i:0 for i in range(18)}\n",
    "    best_val_accuracy = 0\n",
    "    counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_metrics = {'epoch': epoch + 1}\n",
    "\n",
    "        with tqdm(total=len(train_loader) + len(val_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False) as pbar:\n",
    "            for phase in ['train', 'val']:\n",
    "                model.train() if phase == 'train' else model.eval()\n",
    "                data_loader = train_loader if phase == 'train' else val_loader\n",
    "\n",
    "                running_loss, all_preds, all_labels = 0.0, [], []\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    for feats, coords, lidar_inputs, bboxes_list in data_loader:\n",
    "                        labels = [bbox.to(device) for bbox in bboxes_list]\n",
    "                        lidar_inputs['point_clouds'] = lidar_inputs['point_clouds'].to(device)\n",
    "                        coords, feats = ME.utils.sparse_collate(coords, feats)\n",
    "                        in_field = ME.TensorField(\n",
    "                            features = feats,\n",
    "                            coordinates = coords,\n",
    "                            quantization_mode=ME.SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE,\n",
    "                            minkowski_algorithm=ME.MinkowskiAlgorithm.SPEED_OPTIMIZED,\n",
    "                            device=device\n",
    "                        )\n",
    "                        rgb_inputs = in_field.sparse()\n",
    "                        \"\"\"\n",
    "                        rgb_inputs = ME.SparseTensor(features=feats.to(device), coordinates=coords.to(device),  # coordinates must be defined in a integer grid. If the scale\n",
    "                        quantization_mode=ME.SparseTensorQuantizationMode.UNWEIGHTED_AVERAGE)\"\"\"\n",
    "                        \n",
    "                        pred_bb, pred_cl, points = model(rgb_inputs, lidar_inputs)\n",
    "                        \n",
    "                        loss, mAP, AP = criterion(pred_bb, pred_cl, points, labels)\n",
    "                        for ap_per_class in AP:\n",
    "                            for i in range(18):\n",
    "                                if ap_per_class[i] is not None:\n",
    "                                    mean_APs[i] += ap_per_class[i]\n",
    "                            counter += 1\n",
    "                        loss = loss['cls_loss'] + loss['bbox_loss']\n",
    "                        if phase == 'train':\n",
    "                            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "                        running_loss += loss.item() * batch_size\n",
    "                        all_preds.append(mAP)\n",
    "                        pbar.update(1)\n",
    "\n",
    "                epoch_loss = running_loss / len(data_loader.dataset)\n",
    "                epoch_accuracy = np.mean(all_preds)\n",
    "                #epoch_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "                epoch_metrics[f'{phase}_loss'] = epoch_loss\n",
    "                epoch_metrics[f'{phase}_accuracy'] = epoch_accuracy\n",
    "                #epoch_metrics[f'{phase}_f1'] = epoch_f1\n",
    "\n",
    "                if phase == 'val':\n",
    "                    scheduler.step(epoch_loss)\n",
    "                    if epoch_accuracy > best_val_accuracy:\n",
    "                        best_val_accuracy = epoch_accuracy\n",
    "                        torch.save(model.state_dict(), os.path.join(output_folder, \"best_model.pth\"))\n",
    "\n",
    "        writer.add_scalar('Loss/train', epoch_metrics['train_loss'], epoch)\n",
    "        writer.add_scalar('Loss/validation', epoch_metrics['val_loss'], epoch)\n",
    "        writer.add_scalar('Accuracy/train', epoch_metrics['train_accuracy'], epoch)\n",
    "        writer.add_scalar('Accuracy/validation', epoch_metrics['val_accuracy'], epoch)\n",
    "        #writer.add_scalar('F1/train', epoch_metrics['train_f1'], epoch)\n",
    "        #writer.add_scalar('F1/validation', epoch_metrics['val_f1'], epoch)\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame([epoch_metrics])], ignore_index=True)\n",
    "        metrics_df.to_csv(metrics_path, index=False)\n",
    "    for i in range(18):\n",
    "        mean_APs[i] /= counter\n",
    "    print(\"\\nEvaluating best model on test set...\")\n",
    "    model.load_state_dict(torch.load(os.path.join(output_folder, \"best_model.pth\")))\n",
    "    model.eval()\n",
    "\n",
    "    running_loss, all_preds, all_labels = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for feats, coords, lidar_inputs, bboxes_list in data_loader:\n",
    "            labels = [bbox.to(device) for bbox in bboxes_list]\n",
    "            lidar_inputs['point_clouds'] = lidar_inputs['point_clouds'].to(device)\n",
    "            coords, feats = ME.utils.sparse_collate(coords, feats)\n",
    "            rgb_inputs = ME.SparseTensor(features=feats.to(device), coordinates=coords.to(device))\n",
    "            pred_bb, pred_cl, points = model(rgb_inputs, lidar_inputs)\n",
    "            loss, mAP, AP = criterion(pred_bb, pred_cl, points, labels)\n",
    "            loss = loss['bbox_loss'] + loss['cls_loss']\n",
    "            \n",
    "            running_loss += loss.item() * batch_size\n",
    "            all_preds.append(mAP)\n",
    "\n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "    test_accuracy = np.mean(all_preds)\n",
    "    print(pred_bb[0][0].shape)\n",
    "    print(mean_APs)\n",
    "    #test_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "    test_metrics_row = {col: None for col in columns}\n",
    "    test_metrics_row.update({\n",
    "        'epoch': 'best_model',\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        #'test_f1': test_f1\n",
    "    })\n",
    "    \n",
    "    metrics_df = pd.concat([metrics_df, pd.DataFrame([test_metrics_row])], ignore_index=True)\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "    # Save confusion matrix as CSV\n",
    "    \"\"\"conf_matrix = confusion_matrix(all_labels, all_preds, normalize='true')\n",
    "    conf_matrix_csv_path = os.path.join(output_folder, 'normalized_confusion_matrix.csv')\n",
    "    pd.DataFrame(conf_matrix, index=class_names, coluns=class_names).to_csv(conf_matrix_csv_path)\n",
    "    print(f\"Confusion matrix CSV saved at: {conf_matrix_csv_path}\")\"\"\"\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3beacfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'MinkowskiNormalization.MinkowskiBatchNorm'>\n",
      "Loaded pretrained weights from TR3D.pth\n",
      "Loaded pretrained weights from votenet_backbone.pth and votenet_neck.pth\n",
      "Loaded pretrained weights from TR3D.pth\n",
      "Using GPUs: 2\n",
      "Loaded pretrained weights from TR3D.pth\n",
      "tensor([2.4158, 3.9594, 1.2447, 3.6952, 2.5133, 2.0973, 2.8201, 3.9728, 3.1347,\n",
      "        4.3157, 3.3919, 4.0510, 4.4236, 4.9219, 4.3465, 3.7117, 4.9288, 2.0817],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating best model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8947, 6])\n",
      "{0: 0.3068562548544587, 1: 0.0, 2: 0.35974233907946307, 3: 0.0, 4: 0.0, 5: 0.5271795823265206, 6: 0.0, 7: 0.0, 8: 0.17642712067486124, 9: 0.0, 10: 0.0, 11: 0.0, 12: 0.12726695585544634, 13: 0.07554424973368508, 14: 0.1464666646135584, 15: 0.22845055664403818, 16: 0.0, 17: 0.4986162498151042}\n"
     ]
    }
   ],
   "source": [
    "train_model(learning_rate=0.001, num_epochs=20, batch_size=10, voxel_size = 0.02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fusion_env)",
   "language": "python",
   "name": "fusion_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
